import os
import cv2
import torch
import numpy as np
import pickle
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
from torchvision import transforms
from ultralytics import YOLO
from vietocr.tool.predictor import Predictor
from vietocr.tool.config import Cfg
from src.config import (
    FACE_DETECTION_MODEL_PATH, FACENET_MODEL_PATH, FACE_EMBEDDINGS_PATH, CORNER_MODEL_PATH, TEXT_MODEL_PATH, VIETOCR_MODEL_PATH
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def extract_frames(video):
    """
    Tr√≠ch xu·∫•t frame ƒë·∫ßu - gi·ªØa - cu·ªëi m·ªói gi√¢y t·ª´ video.
    
    Args:
        video_path (str): ƒê∆∞·ªùng d·∫´n video.
    
    Returns:
        list: Danh s√°ch frames [(sec, frame_pos, frame)].
    """
    cap = video
    if not cap.isOpened():
        print("[ERROR] Kh√¥ng th·ªÉ m·ªü video!")
        return []

    fps = int(cap.get(cv2.CAP_PROP_FPS))
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = frame_count / fps  # Th·ªùi l∆∞·ª£ng video

    print(f"[INFO] FPS: {fps}, T·ªïng s·ªë frames: {frame_count}, Th·ªùi l∆∞·ª£ng: {duration:.2f} gi√¢y")

    # Ch·ªçn frame ƒë·∫ßu - gi·ªØa - cu·ªëi m·ªói gi√¢y
    frame_selection = [0, fps // 2, fps - 1]
    
    frames = []
    frame_idx = 0

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break

        sec = frame_idx // fps
        frame_pos = frame_idx % fps

        if frame_pos in frame_selection:
            frames.append((sec, frame_pos, frame))

        frame_idx += 1

    cap.release()
    print(f"[INFO] ƒê√£ tr√≠ch xu·∫•t {len(frames)} frames.")
    return frames

def detect(image, model):
    """
    Ph√°t hi·ªán khu√¥n m·∫∑t tr√™n ·∫£nh.

    Args:
        image (numpy.ndarray): ·∫¢nh ƒë·∫ßu v√†o.

    Returns:
        list: Danh s√°ch bbox (x1, y1, x2, y2).
    """
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = model(image_rgb)

    bboxes = []
    for box in results[0].boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        bboxes.append((x1, y1, x2, y2))

    return bboxes

def crop(image, bbox):
    """
    Crop khu√¥n m·∫∑t t·ª´ ·∫£nh theo bbox.

    Args:
        image (numpy.ndarray): ·∫¢nh ƒë·∫ßu v√†o.
        bbox (tuple): (x1, y1, x2, y2) t·ªça ƒë·ªô bbox.

    Returns:
        PIL.Image: ·∫¢nh khu√¥n m·∫∑t ƒë√£ crop.
    """
    x1, y1, x2, y2 = bbox
    face_crop = image[y1:y2, x1:x2]

    if face_crop.size == 0:
        return None

    return Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))

def preprocess_img(face_image):
    """
    Ti·ªÅn x·ª≠ l√Ω ·∫£nh khu√¥n m·∫∑t tr∆∞·ªõc khi ƒë∆∞a v√†o FaceNet.

    Args:
        face_image (PIL.Image): ·∫¢nh khu√¥n m·∫∑t.

    Returns:
        torch.Tensor: Tensor ·∫£nh ƒë√£ chu·∫©n h√≥a.
    """
    transform = transforms.Compose([
        transforms.Resize((160, 160)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    return transform(face_image).unsqueeze(0)

def embed_facenet(face_image, facenet_model):
    """
    Nh·∫≠n di·ªán khu√¥n m·∫∑t v√† t·∫°o vector ƒë·∫∑c tr∆∞ng b·∫±ng FaceNet.

    Args:
        face_image (PIL.Image): ·∫¢nh khu√¥n m·∫∑t.

    Returns:
        numpy.ndarray: Vector ƒë·∫∑c tr∆∞ng (512,)
    """
    face_tensor = preprocess_img(face_image).to(device)
    
    with torch.no_grad():
        embedding = facenet_model(face_tensor).cpu().numpy()

    return embedding.squeeze()

def compute_similarity(vec1, vec2):
    """
    T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng Cosine gi·ªØa hai vector.

    Args:
        vec1 (numpy.ndarray): Vector ƒë·∫∑c tr∆∞ng ƒë·∫ßu ti√™n.
        vec2 (numpy.ndarray): Vector ƒë·∫∑c tr∆∞ng th·ª© hai.

    Returns:
        float: Gi√° tr·ªã cosine similarity.
    """
    vec1 = np.array(vec1).squeeze()
    vec2 = np.array(vec2).squeeze()
    
    return cosine_similarity([vec1], [vec2])[0][0]

def save_embeddings(embeddings):
    """
    L∆∞u embeddings v√†o file.

    Args:
        embeddings (dict): Dictionary ch·ª©a c√°c embeddings.
    """
    with open(FACE_EMBEDDINGS_PATH, "wb") as f:
        pickle.dump(embeddings, f)
    
    print(f"[üíæ SAVED] ƒê√£ l∆∞u {len(embeddings)} embeddings v√†o {FACE_EMBEDDINGS_PATH}")

def load_embeddings():
    """
    T·∫£i embeddings ƒë√£ l∆∞u.

    Returns:
        dict: Dictionary ch·ª©a c√°c embeddings.
    """
    if not os.path.exists(FACE_EMBEDDINGS_PATH):
        print(f"[ERROR] Kh√¥ng t√¨m th·∫•y database embeddings: {FACE_EMBEDDINGS_PATH}")
        return None

    with open(FACE_EMBEDDINGS_PATH, "rb") as f:
        embeddings_data = pickle.load(f)

    return embeddings_data

def get_card_corners(corner_bboxes):
    """
    X√°c ƒë·ªãnh t·ªça ƒë·ªô 4 g√≥c c·ªßa th·∫ª d·ª±a v√†o bbox c·ªßa c√°c corners.

    Args:
        corner_bboxes (dict): Dictionary ch·ª©a bbox c·ªßa c√°c g√≥c.

    Returns:
        dict: Dictionary ch·ª©a t·ªça ƒë·ªô trung t√¢m c·ªßa 4 g√≥c.
    """
    required_corners = {"top_left", "top_right", "bottom_left", "bottom_right"}
    
    if not required_corners.issubset(corner_bboxes.keys()):
        print("‚ùå [ERROR] Kh√¥ng ƒë·ªß 4 g√≥c, y√™u c·∫ßu user upload ·∫£nh kh√°c.")
        return None

    corner_centers = {
        label: ((coords[0] + coords[2]) // 2, (coords[1] + coords[3]) // 2)
        for label, coords in corner_bboxes.items()
    }

    return corner_centers

def transform_perspective(image, corners, output_size=(800, 500)):
    """
    Bi·∫øn ƒë·ªïi ph·ªëi c·∫£nh ·∫£nh CCCD v·ªÅ d·∫°ng chu·∫©n.

    Args:
        image (numpy.ndarray): ·∫¢nh g·ªëc.
        corners (dict): T·ªça ƒë·ªô 4 g√≥c c·ªßa CCCD.
        output_size (tuple): K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu ra.

    Returns:
        numpy.ndarray: ·∫¢nh sau khi transform, ho·∫∑c None n·∫øu l·ªói.
    """
    try:
        if len(corners) != 4:
            raise ValueError("‚ö†Ô∏è Kh√¥ng ƒë·ªß 4 g√≥c ƒë·ªÉ transform.")

        src_points = np.array([
            corners["top_left"],
            corners["top_right"],
            corners["bottom_left"],
            corners["bottom_right"]
        ], dtype=np.float32)

        dst_points = np.array([
            [0, 0],
            [output_size[0] - 1, 0],
            [0, output_size[1] - 1],
            [output_size[0] - 1, output_size[1] - 1]
        ], dtype=np.float32)

        # T√≠nh ma tr·∫≠n bi·∫øn ƒë·ªïi
        M = cv2.getPerspectiveTransform(src_points, dst_points)
        transformed_image = cv2.warpPerspective(image, M, output_size)

        return transformed_image
    except Exception as e:
        print(f"‚ùå [ERROR] {e}")
        return None

def extract_text(image, model):
    """
    Nh·∫≠n di·ªán vƒÉn b·∫£n t·ª´ ·∫£nh ƒë√£ crop.

    Args:
        image (numpy.ndarray): ·∫¢nh v√πng ch·ª©a text.
        model (Predictor): M√¥ h√¨nh OCR.

    Returns:
        str: Chu·ªói k√Ω t·ª± nh·∫≠n di·ªán ƒë∆∞·ª£c.
    """
    try:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_pil = Image.fromarray(image_rgb)

        text = model.predict(image_pil).strip()
        return text
    except Exception as e:
        print(f"‚ùå [ERROR] L·ªói OCR: {e}")
        return ""

def compute_iou(box1, box2):
    """
    T√≠nh to√°n IoU (Intersection over Union) gi·ªØa hai bounding box.

    Args:
        box1 (tuple): (x1, y1, x2, y2) t·ªça ƒë·ªô bbox ƒë·∫ßu ti√™n.
        box2 (tuple): (x1, y1, x2, y2) t·ªça ƒë·ªô bbox th·ª© hai.

    Returns:
        float: Gi√° tr·ªã IoU.
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    iou = inter_area / float(box1_area + box2_area - inter_area)
    return iou

def apply_nms(detections, iou_threshold=0.5):
    """
    √Åp d·ª•ng Non-Maximum Suppression (NMS) ƒë·ªÉ l·ªçc c√°c bbox b·ªã tr√πng.

    Args:
        detections (list): Danh s√°ch bbox [(bbox, confidence, label)].
        iou_threshold (float): Ng∆∞·ª°ng IoU ƒë·ªÉ lo·∫°i b·ªè bbox.

    Returns:
        dict: Dictionary {label: bbox} ch·ª©a bbox t·ªët nh·∫•t cho m·ªói g√≥c.
    """
    print("[INFO] Applying NMS...")
    
    filtered_detections = {}
    unique_labels = set(label for _, _, label in detections)
    
    for label in unique_labels:
        label_detections = [det for det in detections if det[2] == label]
        label_detections.sort(key=lambda x: x[1], reverse=True)  # S·∫Øp x·∫øp theo confidence

        selected_boxes = []
        while label_detections:
            best_box = label_detections.pop(0)
            selected_boxes.append(best_box)
            
            label_detections = [
                box for box in label_detections
                if compute_iou(best_box[0], box[0]) < iou_threshold
            ]

        if selected_boxes:
            filtered_detections[label] = selected_boxes[0][0]
    
    print("[INFO] Filtered detections:", filtered_detections)
    return filtered_detections

def detect_objects(image, model):
    """
    Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (g√≥c CCCD, v√πng vƒÉn b·∫£n) tr√™n ·∫£nh b·∫±ng YOLO.

    Args:
        image (numpy.ndarray): ·∫¢nh ƒë·∫ßu v√†o.
        model (YOLO): Model YOLO ƒë√£ load.

    Returns:
        list: [(bbox, confidence, label), ...]
    """
    results = model(image)
    
    detections = [
        ((int(box.xyxy[0][0]), int(box.xyxy[0][1]), int(box.xyxy[0][2]), int(box.xyxy[0][3])),
         float(box.conf[0]),  # Confidence score
         model.names[int(box.cls[0])])  # Label
        for box in results[0].boxes
    ]
    
    return detections